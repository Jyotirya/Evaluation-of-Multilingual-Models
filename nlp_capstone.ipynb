{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multilingual NLP Capstone: Monolingual vs. Multilingual Models\n",
        "\n",
        "This notebook evaluates and compares two transformer models on sentiment analysis across 8 languages:\n",
        "1. **Monolingual:** `roberta-base` (English only)\n",
        "2. **Multilingual:** `xlm-roberta-base` (multilingual support)\n",
        "\n",
        "The pipeline includes data preprocessing, fine-tuning, evaluation (Accuracy/F1), visualization, and a Gradio demo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers datasets pandas scikit-learn matplotlib seaborn gradio accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import gradio as gr\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import (\n",
        "    RobertaTokenizer,\n",
        "    RobertaForSequenceClassification,\n",
        "    XLMRobertaTokenizer,\n",
        "    XLMRobertaForSequenceClassification,\n",
        "    DataCollatorWithPadding,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    pipeline\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading\n",
        "Upload your `data.zip` file containing the language folders (arabic, english, french, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If running in Colab, uncomment the lines below to upload data\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# !unzip -o data.zip -d /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load datasets\n",
        "# Ensure your data folder structure matches these paths\n",
        "\n",
        "languages = ['arabic', 'english', 'french', 'german', 'italian', 'spanish', 'hindi', 'portuguese']\n",
        "data_path = 'data/' # Adjust if your folder is named differently\n",
        "\n",
        "dfs = {}\n",
        "for lang in languages:\n",
        "    dfs[f'{lang}_train'] = pd.read_json(f'{data_path}{lang}/train.jsonl', lines=True)\n",
        "    dfs[f'{lang}_val'] = pd.read_json(f'{data_path}{lang}/validation.jsonl', lines=True)\n",
        "    dfs[f'{lang}_test'] = pd.read_json(f'{data_path}{lang}/test.jsonl', lines=True)\n",
        "\n",
        "# Combine for Multilingual Training\n",
        "train_dfs = [dfs[f'{lang}_train'] for lang in languages]\n",
        "val_dfs = [dfs[f'{lang}_val'] for lang in languages]\n",
        "\n",
        "df_train = pd.concat(train_dfs, ignore_index=True).sample(frac=1).reset_index(drop=True)\n",
        "df_val = pd.concat(val_dfs, ignore_index=True)\n",
        "\n",
        "# Create Hugging Face Datasets\n",
        "ds_train = Dataset.from_pandas(df_train)\n",
        "ds_val = Dataset.from_pandas(df_val)\n",
        "\n",
        "dataset = DatasetDict({\n",
        "    'train': ds_train,\n",
        "    'validation': ds_val\n",
        "})\n",
        "\n",
        "print(\"Combined Dataset Summary:\", dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Model & Tokenizer Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Model 1: Monolingual (RoBERTa) ---\n",
        "mono_model_name = \"roberta-base\"\n",
        "mono_tokenizer = RobertaTokenizer.from_pretrained(mono_model_name)\n",
        "mono_model = RobertaForSequenceClassification.from_pretrained(mono_model_name, num_labels=3)\n",
        "\n",
        "# --- Model 2: Multilingual (XLM-RoBERTa) ---\n",
        "multi_model_name = \"xlm-roberta-base\"\n",
        "multi_tokenizer = XLMRobertaTokenizer.from_pretrained(multi_model_name)\n",
        "multi_model = XLMRobertaForSequenceClassification.from_pretrained(multi_model_name, num_labels=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text = re.sub(r\"https://\\S+|www\\.\\S+\", \"http\", text)\n",
        "    text = re.sub(r\"@\\w+\", \"@user\", text)\n",
        "    text = re.sub(r\"#\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "# Tokenization Functions\n",
        "def clean_and_tokenize_mono(batch):\n",
        "    cleaned_texts = [clean_text(text) for text in batch[\"text\"]]\n",
        "    return mono_tokenizer(cleaned_texts, truncation=True)\n",
        "\n",
        "def clean_and_tokenize_multi(batch):\n",
        "    cleaned_texts = [clean_text(text) for text in batch[\"text\"]]\n",
        "    return multi_tokenizer(cleaned_texts, truncation=True)\n",
        "\n",
        "# Apply Tokenization\n",
        "# 1. Monolingual (English Only)\n",
        "dataset_mono = DatasetDict({\n",
        "    'train': Dataset.from_pandas(dfs['english_train']),\n",
        "    'validation': Dataset.from_pandas(dfs['english_val'])\n",
        "})\n",
        "tokenized_dataset_mono = dataset_mono.map(clean_and_tokenize_mono, batched=True)\n",
        "\n",
        "# 2. Multilingual (All Languages)\n",
        "tokenized_dataset_multi = dataset.map(clean_and_tokenize_multi, batched=True)\n",
        "\n",
        "# Data Collators\n",
        "data_collator_mono = DataCollatorWithPadding(tokenizer=mono_tokenizer)\n",
        "data_collator_multi = DataCollatorWithPadding(tokenizer=multi_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    f1 = f1_score(labels, predictions, average=\"macro\")\n",
        "    return {\"accuracy\": acc, \"f1\": f1}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Train Monolingual Model ---\n",
        "training_args_mono = TrainingArguments(\n",
        "    output_dir=\"./results_mono\",\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=100,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer_mono = Trainer(\n",
        "    model=mono_model,\n",
        "    args=training_args_mono,\n",
        "    train_dataset=tokenized_dataset_mono[\"train\"],\n",
        "    eval_dataset=tokenized_dataset_mono[\"validation\"],\n",
        "    data_collator=data_collator_mono,\n",
        "    tokenizer=mono_tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(\"Starting Monolingual Training...\")\n",
        "trainer_mono.train()\n",
        "trainer_mono.save_model(\"./models/roberta-base-finetuned\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Train Multilingual Model ---\n",
        "training_args_multi = TrainingArguments(\n",
        "    output_dir=\"./results_multi\",\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=100,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer_multi = Trainer(\n",
        "    model=multi_model,\n",
        "    args=training_args_multi,\n",
        "    train_dataset=tokenized_dataset_multi[\"train\"],\n",
        "    eval_dataset=tokenized_dataset_multi[\"validation\"],\n",
        "    data_collator=data_collator_multi,\n",
        "    tokenizer=multi_tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(\"Starting Multilingual Training...\")\n",
        "trainer_multi.train()\n",
        "trainer_multi.save_model(\"./models/xlm-roberta-base-finetuned\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Evaluation & Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = []\n",
        "labels_list = [\"Negative\", \"Neutral\", \"Positive\"]\n",
        "\n",
        "print(\"--- Evaluating models language-wise ---\")\n",
        "\n",
        "for lang in languages:\n",
        "    test_df = dfs[f'{lang}_test']\n",
        "    ds_test = Dataset.from_pandas(test_df)\n",
        "    \n",
        "    # Preprocess\n",
        "    tok_test_mono = ds_test.map(clean_and_tokenize_mono, batched=True)\n",
        "    tok_test_multi = ds_test.map(clean_and_tokenize_multi, batched=True)\n",
        "    true_labels = ds_test['label']\n",
        "\n",
        "    # 1. Monolingual Preds\n",
        "    mono_out = trainer_mono.predict(tok_test_mono)\n",
        "    mono_preds = np.argmax(mono_out.predictions, axis=1)\n",
        "    mono_f1 = f1_score(true_labels, mono_preds, average=\"macro\")\n",
        "    results.append({\"language\": lang, \"model\": \"roberta-base\", \"f1\": mono_f1})\n",
        "\n",
        "    # 2. Multilingual Preds\n",
        "    multi_out = trainer_multi.predict(tok_test_multi)\n",
        "    multi_preds = np.argmax(multi_out.predictions, axis=1)\n",
        "    multi_f1 = f1_score(true_labels, multi_preds, average=\"macro\")\n",
        "    results.append({\"language\": lang, \"model\": \"xlm-roberta-base\", \"f1\": multi_f1})\n",
        "\n",
        "    # Plot Confusion Matrix for English and Hindi as examples\n",
        "    if lang in ['english', 'hindi']:\n",
        "        fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
        "        sns.heatmap(confusion_matrix(true_labels, mono_preds), annot=True, fmt='d', cmap='Blues', ax=ax[0])\n",
        "        ax[0].set_title(f'RoBERTa on {lang}')\n",
        "        \n",
        "        sns.heatmap(confusion_matrix(true_labels, multi_preds), annot=True, fmt='d', cmap='Oranges', ax=ax[1])\n",
        "        ax[1].set_title(f'XLM-RoBERTa on {lang}')\n",
        "        plt.show()\n",
        "\n",
        "# Create Results DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot F1 Comparison\n",
        "plt.figure(figsize=(14, 7))\n",
        "sns.barplot(x=\"language\", y=\"f1\", hue=\"model\", data=results_df, palette=[\"#6495ED\", \"#FF6347\"])\n",
        "plt.title(\"F1-Score: Monolingual vs Multilingual Models\")\n",
        "plt.ylim(0, 1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Gradio Demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load best model for inference\n",
        "model_path = \"./models/xlm-roberta-base-finetuned\"\n",
        "classifier = pipeline(\"sentiment-analysis\", model=model_path, tokenizer=model_path)\n",
        "\n",
        "# Map ids to labels\n",
        "id2label = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
        "classifier.model.config.id2label = id2label\n",
        "classifier.model.config.label2id = {v: k for k, v in id2label.items()}\n",
        "\n",
        "def predict(text):\n",
        "    text = clean_text(text)\n",
        "    res = classifier(text)[0]\n",
        "    return {res['label']: res['score']}\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=predict,\n",
        "    inputs=gr.Textbox(placeholder=\"Enter text in any supported language...\"),\n",
        "    outputs=gr.Label(num_top_classes=3),\n",
        "    title=\"Multilingual Sentiment Analysis\",\n",
        "    description=\"Detects Positive, Neutral, or Negative sentiment in 8 languages.\"\n",
        ")\n",
        "\n",
        "iface.launch()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}